{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data4/haryoaw_workspace/projects/2021_2/s4_happy/happy_s4\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/shared_envs/s4/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/mnt/data1/hf_dataset_cache/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "mrpcc_data = datasets.load_dataset(\"glue\", name=\"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    \n",
    "    def __init__(self, special_tokens: List[str] = None, min_freq: int = 1, \n",
    "                 split_pattern: str = r\"\\s+\", unk_token: str = \"[UNK]\", pad_token: str = '[PAD]',\n",
    "                 eos_token: str = \"[EOS]\", sos_token: str=\"[SOS]\",\n",
    "                 lowercase: bool = False):\n",
    "        \"\"\"\n",
    "        Word Tokenizer\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lowercase: bool\n",
    "            Lowercase?\n",
    "        special_tokens: List[str]\n",
    "            List of special tokens\n",
    "        min_freq: int\n",
    "            Minimum frequency of the token, by default = 1\n",
    "        split_pattern: str\n",
    "            Tokenizer pattern to distinguish the tokens, by default `\\s+` (space)\n",
    "        \"\"\"\n",
    "        self.pad_token = pad_token.lower() if lowercase else pad_token\n",
    "        self.unk_token = unk_token.lower() if lowercase else unk_token\n",
    "        self.sos_token = sos_token.lower() if lowercase else sos_token\n",
    "        self.eos_token = eos_token.lower() if lowercase else eos_token\n",
    "        \n",
    "        sp_token_default = [pad_token, unk_token, eos_token, sos_token] if not lowercase else [pad_token.lower(), unk_token.lower(), eos_token.lower(), sos_token.lower()]\n",
    "        self.special_tokens = sp_token_default if special_tokens is None else sp_token_default + special_tokens\n",
    "        self.min_freq = min_freq\n",
    "        self.split_pattern = split_pattern\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "        # Fit variables\n",
    "        self.vocab = None\n",
    "        self.pad_token_id = None\n",
    "        self.unk_token_id = None\n",
    "        self.eos_token_id = None\n",
    "        self.sos_token_id = None\n",
    "        \n",
    "    def fit(self, *data: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Fit data's vocabulary\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: List[str]\n",
    "            The dataset \n",
    "        \"\"\"\n",
    "        all_tokens = []\n",
    "        for dt in data:\n",
    "            for x in dt:\n",
    "                x = x.lower() if self.lowercase else x\n",
    "                all_tokens += re.split(self.split_pattern, x)\n",
    "        self.vocab = vocab(\n",
    "            Counter(all_tokens), \n",
    "            min_freq=self.min_freq, \n",
    "            specials=self.special_tokens,\n",
    "        )\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "        self.eos_token_id = self.vocab[self.eos_token]\n",
    "        self.sos_token_id = self.vocab[self.sos_token]\n",
    "    \n",
    "    def tokenize_to_ids(self, inp: str, with_eos_sos: bool = False) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize a text input to its input indices\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp: str\n",
    "            Text input\n",
    "        with_eos_sos: bool\n",
    "            Concate it with end of sentence and start of sentence token?\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            tokenized text (indices)\n",
    "        \"\"\"\n",
    "        inp_ready = inp.lower() if self.lowercase else inp\n",
    "        splitted_txt = re.split(self.split_pattern, inp_ready)\n",
    "        ids_splitted_txt = [self.vocab[x] if x in self.vocab else self.vocab[self.unk_token] for x in splitted_txt]\n",
    "        if with_eos_sos:\n",
    "            ids_splitted_txt = [self.sos_token_id]  + ids_splitted_txt + [self.eos_token_id]\n",
    "        return ids_splitted_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = WordTokenizer(lowercase=True, special_tokens=['[sep]', '[cls]'])\n",
    "wt.fit(mrpcc_data['sentence1'], mrpcc_data['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.tokenize_to_ids(\"[sep]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modelling :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_dataset(inp_dict, word_tokenizer, text_cols = ['sentence1', 'sentence2'], label_col='label'):\n",
    "    input_ids = word_tokenizer.tokenize_to_ids('[CLS]')\n",
    "    for col in text_cols:\n",
    "        input_ids += word_tokenizer.tokenize_to_ids(inp_dict[col])\n",
    "        input_ids += word_tokenizer.tokenize_to_ids('[SEP]')\n",
    "    returned_dict = dict(label=inp_dict[label_col], input_ids=input_ids)\n",
    "    return returned_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ex = {\n",
    "    'sentence1': \"hi my name is bejo\",\n",
    "    'sentence2': \"Meong Meong\",\n",
    "    'label': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0, 'input_ids': [5, 1, 2634, 1286, 281, 1, 4, 1, 1, 4]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_dataset(inp_ex, word_tokenizer=wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3668/3668 [00:00<00:00, 6145.25ex/s]\n"
     ]
    }
   ],
   "source": [
    "mrpcc_data = mrpcc_data.map(partial(shape_dataset, word_tokenizer=wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence1', 'sentence2', 'label', 'idx', 'input_ids']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrpcc_data.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpcc_data = mrpcc_data.remove_columns(['sentence1', 'sentence2', 'idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCollators:\n",
    "    \n",
    "    def __init__(self, pad_token_ids):\n",
    "        self.pad_token_ids = pad_token_ids\n",
    "        self.pad_strategy = 'max_length'\n",
    "    \n",
    "    def _pad_helper(self, x, max_length):\n",
    "        cur_len = len(x)\n",
    "        added_pad_len = max_length - cur_len\n",
    "        padded_x = x + [self.pad_token_ids] * added_pad_len\n",
    "        return padded_x\n",
    "\n",
    "    def _pad_self(self, input_ids):\n",
    "        max_length = max(map(len, input_ids))\n",
    "        padded_input_ids = [self._pad_helper(x, max_length) for x in input_ids]\n",
    "        return padded_input_ids\n",
    "            \n",
    "    def __call__(self, inp):\n",
    "        input_ids, labels = [], []\n",
    "        for i in inp:\n",
    "            input_ids.append(i['input_ids'])\n",
    "            labels.append(i['label'])\n",
    "        input_ids = self._pad_self(input_ids)\n",
    "        return {\n",
    "            'input_ids': torch.LongTensor(input_ids),\n",
    "            'label': torch.LongTensor(labels)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset=mrpcc_data, collate_fn=BatchCollators(wt.pad_token_id), batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happy_s4.model.s4 import S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class S4_GO_BRR_ARGS:\n",
    "    pad_token_id: int\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    l_max: int\n",
    "    channels: int\n",
    "    bidirectional: bool = True\n",
    "    trainable: bool = True\n",
    "    lr: float = 0.001\n",
    "    tie_state: bool = True\n",
    "    hurwitz: bool = True,\n",
    "    transposed: bool = True\n",
    "    pool: str = \"last\"\n",
    "        \n",
    "    def get_s4_args(self):\n",
    "        args = ['d_model', 'l_max', 'channels', 'bidirectional', 'trainable', 'lr', 'tie_state',\n",
    "                'hurwitz', 'transposed']\n",
    "        dict_returned = {arg: self.__dict__[arg] for arg in args}\n",
    "        return dict_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = {\"dt\": True, \"A\": True, \"P\": True, \"B\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = wt.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wt.vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = S4_GO_BRR_ARGS(pad_token_id=pad_token_id, \n",
    "          vocab_size=vocab_size,\n",
    "          d_model=128, l_max=512, channels=1, bidirectional=True, trainable=trainable, lr=0.001, tie_state=True, hurwitz=True, transposed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4_GO_BRR(Module):\n",
    "    \n",
    "    def __init__(self, args: S4_GO_BRR_ARGS):\n",
    "        super().__init__()\n",
    "        self.s4 = S4(**args.get_s4_args())\n",
    "        self.args=args\n",
    "        self.embedding = Embedding(num_embeddings=args.vocab_size, embedding_dim=args.d_model, padding_idx=args.pad_token_id)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        s4_out = self.embedding(input_ids)\n",
    "        forward_s4 = self.s4(s4_out)\n",
    "        if self.args.pool == \"last\":\n",
    "            pooled_out = forward_s4[0][:,-1]\n",
    "        return pooled_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4_GO_BRR_Classification(Module):\n",
    "    \n",
    "    def __init__(self, args: S4_GO_BRR_ARGS):\n",
    "        super().__init__()\n",
    "        self.backbone = S4_GO_BRR(args)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(self, input_ids, labels):\n",
    "        s4_out = self.backbone(input_ids)\n",
    "        loss = F.cross_entropy(s4_out, labels)\n",
    "        return loss, s4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = S4_GO_BRR(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = S4_GO_BRR_Classification(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, out = model(input_ids, batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 55])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4",
   "language": "python",
   "name": "s4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
